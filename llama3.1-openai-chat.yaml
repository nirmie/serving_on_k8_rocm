apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama3
spec:
  predictor:
    containers:
    - args:
        - "--served-model-name"
        - "llama3"
        - "--port"
        - "8080"
        - "--model"
        - "meta-llama/Meta-Llama-3.1-70B"
        - "--chat-template"
        - "examples/template_chatml.jinja"
        - "--tensor-parallel-size"
        - "4"
        - "--api-key"
        - "token-abc123"
      command:
        - "python3"
        - "-m"
        - "vllm.entrypoints.openai.api_server"
      env:
        - name: HUGGING_FACE_HUB_TOKEN
          value: "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"
#        - name: NCCL_P2P_DISABLE
#          value: "1"
      ports:
        - containerPort: 8080
      image: nirmalsenthil/vllm-rocm
      imagePullPolicy: IfNotPresent
      name: vllm-container
      resources:
        limits:
          cpu: "4"
          memory: 400Gi
          amd.com/gpu: "4"
        requests:
          cpu: "1"
          memory: 200Gi
          amd.com/gpu: "4"
